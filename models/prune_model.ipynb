{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def encode(series):\n",
    "    return pd.get_dummies(series.astype(str))\n",
    "def mean_absolute_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(np.squeeze(y_true)), np.array(np.squeeze(y_pred))\n",
    "    return np.mean(np.abs((y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model('cnnc_iter3.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model.layers[0].get_weights()[0]\n",
    "w2 = model.layers[3].get_weights()[0]\n",
    "w3 = model.layers[6].get_weights()[0]\n",
    "#w4 = model.layers[6].get_weights()[0]\n",
    "#w5 = model.layers[9].get_weights()[0]\n",
    "#w6 = model.layers[10].get_weights()[0]\n",
    "#w4 = model.layers[12].get_weights()[0]\n",
    "\n",
    "conv_layer_weights = [w1,w2,w3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = np.array(np.squeeze(abs(w4)))\n",
    "fc_inds = np.argsort(w4)\n",
    "sorted_w4 = w4[fc_inds]\n",
    "\n",
    "plt.plot(sorted_w4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filt_vec = np.zeros((6,256))\n",
    "\n",
    "for i in range(len(conv_layer_weights)):\n",
    "    weight=conv_layer_weights[i]\n",
    "    weights_dict = {}\n",
    "    num_filters = len(weight[0,0,0,:])\n",
    "    \n",
    "    for j in range(num_filters):\n",
    "        ws = np.sum(abs(weight[:,:,:,j]))\n",
    "        filt = '{}'.format(j)\n",
    "        weights_dict[filt]=ws\n",
    "    \n",
    "    weights_dict_sort = sorted(weights_dict.items(),key=lambda kv:kv[1])\n",
    "    print('L1 norm conv layer{}/n'.format(i+1),weights_dict_sort)\n",
    "    print(0.1*float(weights_dict_sort[-1][1]))\n",
    "    \n",
    "    weights_value = []\n",
    "    n = 0\n",
    "    for elem in weights_dict_sort:\n",
    "        weights_value.append(elem[1])\n",
    "        filt_vec[i,n] = elem[0]\n",
    "        n = n+1\n",
    "        \n",
    "    xc = range(num_filters)\n",
    "    \n",
    "    plt.figure(i+1,figsize=(8,5))\n",
    "    plt.plot(xc, weights_value)\n",
    "    plt.xlabel('Filter Number')\n",
    "    plt.ylabel('L1 Norm')\n",
    "    plt.title('CNN-c Convolutional Layer {}'.format(i+1))\n",
    "    plt.grid(True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerassurgeon import identify, Surgeon\n",
    "from kerassurgeon.operations import delete_channels, delete_layer\n",
    "lr = 0.0001\n",
    "loss='mean_squared_error'\n",
    "optimizer = keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = []\n",
    "for i in range(len(filt_vec[0,0:2])):\n",
    "    li1.append(int(filt_vec[0,i]))\n",
    "\n",
    "li2 = []\n",
    "for i in range(len(filt_vec[1,0:10])):\n",
    "    li2.append(int(filt_vec[1,i]))\n",
    "\n",
    "li3 = []\n",
    "for i in range(len(filt_vec[2,0:50])):\n",
    "    li3.append(int(filt_vec[2,i]))\n",
    "#li4 = []\n",
    "#for i in range(len(filt_vec[3,0:4])):\n",
    "#    li4.append(int(filt_vec[3,i]))\n",
    "\n",
    "#li5 = []\n",
    "#for i in range(len(filt_vec[4,0:6])):\n",
    "#    li5.append(int(filt_vec[4,i]))\n",
    "#li6 = []\n",
    "#for i in range(len(filt_vec[5,0:4])):\n",
    "#    li6.append(int(filt_vec[5,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model.layers[1]\n",
    "layer2 = model.layers[4]\n",
    "layer3 = model.layers[7]\n",
    "#layer4 = model.layers[6]\n",
    "#layer5 = model.layers[9]\n",
    "#layer6 = model.layers[10]\n",
    "\n",
    "surgeon = Surgeon(model)\n",
    "#surgeon.add_job('delete_channels',layer1,channels=li1)\n",
    "surgeon.add_job('delete_channels',layer2,channels=li2)\n",
    "#surgeon.add_job('delete_channels',layer3,channels=li3)\n",
    "#surgeon.add_job('delete_channels',layer4,channels=li4)\n",
    "#surgeon.add_job('delete_channels',layer5,channels=li5)\n",
    "#surgeon.add_job('delete_channels',layer6,channels=li6)\n",
    "\n",
    "model_new = surgeon.operate()\n",
    "model_new.compile(optimizer,loss, metrics=['mean_absolute_error'])\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Classification testing\n",
    "#Import label data\n",
    "\n",
    "temp_labels = np.loadtxt('vec_mat_clabels_icex_src_0.01train.csv',delimiter=\",\")\n",
    "labels_t = []\n",
    "\n",
    "for l in range(len(temp_labels)):\n",
    "    labels_t.append(str(temp_labels[l]))\n",
    "\n",
    "labels_t = np.array(labels_t)\n",
    "labels_t = labels_t.ravel()\n",
    "\n",
    "def encode(series):\n",
    "    return pd.get_dummies(series.astype(str))\n",
    "\n",
    "y_train = encode(labels_t)\n",
    "labels = list(y_train.columns.values)\n",
    "\n",
    "#Import test data\n",
    "features_test = np.loadtxt('vec_mat_features_icex_src_test2_norm_m10db.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_icex_src_test2.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "real_test = features_test[:,0::2]\n",
    "imag_test = features_test[:,1::2]\n",
    "X_test = np.zeros((features_test.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features_test.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_test[k,i,j,0] = real_test[k,count]\n",
    "            X_test[k,i,j,1] = imag_test[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_test[k,j,i,0] = X_test[k,i,j,0]\n",
    "                X_test[k,j,i,1] = -X_test[k,i,j,1]\n",
    "            \n",
    "            count = count + 1\n",
    "\n",
    "\n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "y_test = temp_ytest\n",
    "\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "pred_labels = []\n",
    "for i in np.argmax(predictions, axis=1):\n",
    "    pred_labels.append(float(labels[i]))\n",
    "    \n",
    "    \n",
    "#Classification acc\n",
    "mae = mean_absolute_error(y_test, pred_labels)\n",
    "diff = abs((np.transpose(pred_labels))-(y_test))\n",
    "error = diff[diff>1]\n",
    "percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "\n",
    "print('Test mae:', mae)\n",
    "print('Testing percent within 1km:', percent_correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN Regression Testing\n",
    "\n",
    "features_test = np.loadtxt('vec_mat_features_icex_src_test2_norm.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_icex_src_test2.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "real_test = features_test[:,0::2]\n",
    "imag_test = features_test[:,1::2]\n",
    "X_test = np.zeros((features_test.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features_test.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_test[k,i,j,0] = real_test[k,count]\n",
    "            X_test[k,i,j,1] = imag_test[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_test[k,j,i,0] = X_test[k,i,j,0]\n",
    "                X_test[k,j,i,1] = -X_test[k,i,j,1]\n",
    "            \n",
    "            count = count + 1\n",
    "\n",
    "    #X_test[k,:,:,0] = X_test[k,:,:,0]/np.amax(np.abs(X_test[k,:,:,0]))\n",
    "    #X_test[k,:,:,1] = X_test[k,:,:,1]/np.amax(np.abs(X_test[k,:,:,1]))\n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "y_test = temp_ytest\n",
    "\n",
    "predictions = model_new.predict(X_test)\n",
    "mae = mean_absolute_error(y_test,predictions);\n",
    "print('Test mae:', mae)\n",
    "\n",
    "diff = abs((np.transpose(predictions))-(y_test))\n",
    "error = diff[diff>1]\n",
    "percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "print('Percent within 1km:', percent_correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(np.array(np.squeeze(y_test))-np.array(np.squeeze(predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import glorot_uniform  # Or your initializer of choice\n",
    "import keras.backend as K\n",
    "\n",
    "initial_weights = model_new.get_weights()\n",
    "\n",
    "backend_name = K.backend()\n",
    "if backend_name == 'tensorflow': \n",
    "    k_eval = lambda placeholder: placeholder.eval(session=K.get_session())\n",
    "elif backend_name == 'theano': \n",
    "    k_eval = lambda placeholder: placeholder.eval()\n",
    "else: \n",
    "    raise ValueError(\"Unsupported backend\")\n",
    "\n",
    "new_weights = [k_eval(glorot_uniform()(w.shape)) for w in initial_weights]\n",
    "\n",
    "model_new.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.save('iter2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "class traininghist(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        self.trainingloss = []\n",
    "        self.trainingmape = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, mape = self.model.evaluate(x, y, verbose=0)\n",
    "        self.trainingloss.append(loss)\n",
    "        self.trainingmape.append(mape)\n",
    "        print('Training loss: {}, mape: {}\\n'.format(loss, mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training data\n",
    "\n",
    "features = np.loadtxt('vec_mat_features_icex_src_0.01train_norm.csv',delimiter=\",\")\n",
    "labels_unstand = np.loadtxt('vec_mat_rlabels_icex_src_0.01train.csv',delimiter=\",\")\n",
    "labels_t = []\n",
    "\n",
    "real = features[:,0::2]\n",
    "imag = features[:,1::2]\n",
    "X_train = np.zeros((features.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_train[k,i,j,0] = real[k,count]\n",
    "            X_train[k,i,j,1] = imag[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_train[k,j,i,0] = X_train[k,i,j,0]\n",
    "                X_train[k,j,i,1] = -X_train[k,i,j,1]\n",
    "                \n",
    "            count = count + 1\n",
    "    \n",
    "    #X_train[k,:,:,0] = X_train[k,:,:,0]/np.amax(np.abs(X_train[k,:,:,0]))\n",
    "    #X_train[k,:,:,1] = X_train[k,:,:,1]/np.amax(np.abs(X_train[k,:,:,1]))        \n",
    "\n",
    "labels_unstand = labels_unstand.ravel()\n",
    "#y_train,mu,sigma_labels = std_y(labels_unstand)\n",
    "\n",
    "y_train = labels_unstand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test data\n",
    "\n",
    "features_test = np.loadtxt('vec_mat_features_icex_src_test2_norm.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_icex_src_test2.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "real_test = features_test[:,0::2]\n",
    "imag_test = features_test[:,1::2]\n",
    "X_test = np.zeros((features_test.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features_test.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_test[k,i,j,0] = real_test[k,count]\n",
    "            X_test[k,i,j,1] = imag_test[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_test[k,j,i,0] = X_test[k,i,j,0]\n",
    "                X_test[k,j,i,1] = -X_test[k,i,j,1]\n",
    "                \n",
    "            count = count + 1\n",
    "    \n",
    "    #X_test[k,:,:,0] = X_test[k,:,:,0]/np.amax(np.abs(X_test[k,:,:,0]))\n",
    "    #X_test[k,:,:,1] = X_test[k,:,:,1]/np.amax(np.abs(X_test[k,:,:,1])) \n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "#y_test = (temp_ytest - mu)/sigma_labels\n",
    "y_test = temp_ytest\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "index=np.arange(X_train.shape[0])\n",
    "np.random.shuffle(index)\n",
    "\n",
    "X_train=X_train[index,:,:,:]\n",
    "y_train=y_train[index]\n",
    "\n",
    "drate = 0.25\n",
    "n_node = 256\n",
    "batch_size = 128\n",
    "loss='mean_squared_error'\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = keras.optimizers.Adam(lr)\n",
    "\n",
    "filepath = \"temp.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, mode='auto',period=1)\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=25, mode='auto')\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=40, mode='auto',restore_best_weights=True)\n",
    "traininghistory = traininghist((X_train,y_train))\n",
    "callbacks_list = [checkpoint,reduce,traininghistory,early]\n",
    "\n",
    "infdb = model_new.fit(X_train, y_train, batch_size,verbose = True, epochs=1000, validation_split=0.2, shuffle=True,callbacks=callbacks_list)\n",
    "\n",
    "#Testing\n",
    "\n",
    "test_loss, test_acc = model_new.evaluate(X_test,y_test)\n",
    "train_loss, train_acc = model_new.evaluate(X_train,y_train)\n",
    "    \n",
    "predictions = model_new.predict(X_test)\n",
    "diff = abs((np.transpose(predictions))-(y_test))\n",
    "error = diff[diff>1]\n",
    "percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "    \n",
    "predictions_train = model_new.predict(X_train)\n",
    "diff = abs((np.transpose(predictions_train))-(y_train))\n",
    "error = diff[diff>1]\n",
    "percent_correct_train = (len(y_train)-len(error))/len(y_train)\n",
    "    \n",
    "print('Training mape:', train_acc)\n",
    "print('Test mape:', test_acc)\n",
    "print('Training percent within 1km:',percent_correct_train*100)\n",
    "print('Testing percent within 1km:', percent_correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
