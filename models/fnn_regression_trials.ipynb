{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class traininghist(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        self.trainingloss = []\n",
    "        self.trainingmape = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, mape = self.model.evaluate(x, y, verbose=0)\n",
    "        self.trainingloss.append(loss)\n",
    "        self.trainingmape.append(mape)\n",
    "        print('Training loss: {}, mape: {}\\n'.format(loss, mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('vec_mat_features_icex_src_0.01train_norm.csv',delimiter=\",\")\n",
    "labels_unstand = np.loadtxt('vec_mat_rlabels_icex_src_0.01train.csv',delimiter=\",\")\n",
    "\n",
    "#Import training data\n",
    "X_train = preprocessing.scale(X_train)\n",
    "labels_unstand = labels_unstand.ravel()\n",
    "y_train = labels_unstand\n",
    "\n",
    "index=np.arange(X_train.shape[0])\n",
    "np.random.shuffle(index)\n",
    "\n",
    "X_train=X_train[index,:]\n",
    "y_train=y_train[index]\n",
    "\n",
    "#X_train,y_train = shuffle(X_train, y_train)\n",
    "\n",
    "#Import testing data\n",
    "X_test = np.loadtxt('vec_mat_features_icex_src_test2_norm.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_icex_src_test2_norm.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "X_test = preprocessing.scale(X_test)\n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "y_test = temp_ytest\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing\n",
    "\n",
    "drate = 0.25\n",
    "n_node = 256\n",
    "batch_size = 128\n",
    "loss='mean_squared_error'\n",
    "\n",
    "pc_train=[]\n",
    "pc_test=[]\n",
    "train_mape=[]\n",
    "test_mape=[]\n",
    "\n",
    "for t in range(1):\n",
    "    print(t)\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(units=n_node, activation='selu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(drate),\n",
    "        keras.layers.Dense(units=n_node, activation='selu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(drate),\n",
    "        keras.layers.Dense(units=n_node, activation='selu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(drate),\n",
    "        keras.layers.Dense(units=n_node, activation='selu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(units=n_node, activation='selu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(units=1, activation='linear')])\n",
    "\n",
    "    lr = 0.001\n",
    "    optimizer = keras.optimizers.Adam(lr)\n",
    "\n",
    "    model.compile(optimizer,loss, metrics=['mape'])\n",
    "\n",
    "    filepath = \"temp.h5\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, mode='auto',period=1,verbose=True)\n",
    "    reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto',verbose=True)\n",
    "    early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=15, mode='auto',restore_best_weights=True)\n",
    "    \n",
    "    traininghistory = traininghist((X_train,y_train))\n",
    "    callbacks_list = [checkpoint,reduce,traininghistory,early]\n",
    "    \n",
    "    infdb = model.fit(X_train, y_train, batch_size, verbose = True, epochs=500, validation_split=0.2, shuffle=True, callbacks=callbacks_list)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "    train_loss, train_acc = model.evaluate(X_train,y_train)\n",
    "    \n",
    "    train_mape.append(train_acc)\n",
    "    test_mape.append(test_acc)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    diff = abs((np.transpose(predictions))-(y_test))\n",
    "    error = diff[diff>1]\n",
    "    percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "    \n",
    "    pc_test.append(percent_correct)\n",
    "    \n",
    "    predictions_train = model.predict(X_train)\n",
    "    diff = abs((np.transpose(predictions_train))-(y_train))\n",
    "    error = diff[diff>1]\n",
    "    percent_correct_train = (len(y_train)-len(error))/len(y_train)\n",
    "    \n",
    "    pc_train.append(percent_correct_train)\n",
    "    \n",
    "print('Training mape:', np.mean(train_mape),'+/-', np.std(train_mape))\n",
    "print('Test mape:', np.mean(test_mape),'+/-', np.std(test_mape))\n",
    "print('Training percent within 1km:', np.mean(pc_train)*100,'+/-', np.std(pc_train)*100)\n",
    "print('Testing percent within 1km:', np.mean(pc_test)*100,'+/-', np.std(pc_test)*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "epoch = np.arange(1,len(infdb.history['loss'])+1)\n",
    "fig1 = plt.figure(figsize=(8, 4))\n",
    "plt.suptitle('5-Layer FNN Regression (With Normalization)',fontsize=20)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch, traininghistory.trainingloss, label = 'Training')\n",
    "plt.plot(epoch, infdb.history['val_loss'],label = 'Testing')\n",
    "plt.legend(fontsize=15)\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Number',fontsize=20)\n",
    "plt.ylabel('MSE Loss',fontsize=20)\n",
    "plt.axis([0, len(infdb.history['loss'])+1, 0, 400],fontsize=20)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, np.array(traininghistory.trainingmape),label = 'Training')\n",
    "plt.plot(epoch, np.array(infdb.history['val_mean_absolute_percentage_error']),label = 'Testing')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Epoch Number',fontsize=20)\n",
    "plt.ylabel('MAPE',fontsize=20)\n",
    "plt.grid()\n",
    "plt.axis([0, len(infdb.history['loss'])+1, 0, 100],fontsize=20)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig('fnn5l_reg_norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "\n",
    "inds = y_test.argsort()\n",
    "sorted_y_test = y_test[inds]\n",
    "\n",
    "sorted_y_pred = np.squeeze(predictions)[inds]\n",
    "\n",
    "fig1 = plt.figure(figsize = (20,14))\n",
    "plt.scatter(sorted_y_test,sorted_y_pred)\n",
    "plt.axis([3, 50, 3, 50])\n",
    "plt.ylabel('Predicted Distance (km)',fontsize=20)\n",
    "plt.xlabel('Actual Distance (km)',fontsize=20)\n",
    "plt.title('5-Layer FNN with 512 nodes (SNR = INF)',fontsize=20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
