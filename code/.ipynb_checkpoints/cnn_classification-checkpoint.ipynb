{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training data\n",
    "\n",
    "features = np.loadtxt('vec_mat_features_icex_src_0.01train.csv',delimiter=\",\")\n",
    "temp_labels = np.loadtxt('vec_mat_clabels_icex_src_0.01train.csv',delimiter=\",\")\n",
    "labels_t = []\n",
    "\n",
    "real = features[:,0::2]\n",
    "imag = features[:,1::2]\n",
    "X_train = np.zeros((features.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_train[k,i,j,0] = real[k,count]\n",
    "            X_train[k,i,j,1] = imag[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_train[k,j,i,0] = X_train[k,i,j,0]\n",
    "                X_train[k,j,i,1] = -X_train[k,i,j,1]\n",
    "                \n",
    "            count = count + 1\n",
    "    \n",
    "    X_train[k,:,:,0] = X_train[k,:,:,0]/np.amax(np.abs(X_train[k,:,:,0]))\n",
    "    X_train[k,:,:,1] = X_train[k,:,:,1]/np.amax(np.abs(X_train[k,:,:,1]))                 \n",
    "      \n",
    "\n",
    "for l in range(len(temp_labels)):\n",
    "    labels_t.append(str(temp_labels[l]))\n",
    "    \n",
    "labels_t = np.array(labels_t)\n",
    "labels_t = labels_t.ravel()\n",
    "\n",
    "def encode(series): \n",
    "    return pd.get_dummies(series.astype(str))\n",
    "\n",
    "y_train = encode(labels_t)\n",
    "labels = list(y_train.columns.values)\n",
    "\n",
    "y_train = pd.DataFrame.as_matrix(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test data\n",
    "\n",
    "features_test = np.loadtxt('vec_mat_features_icex_src_test2.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_clabels_icex_src_test2.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "real_test = features_test[:,0::2]\n",
    "imag_test = features_test[:,1::2]\n",
    "X_test = np.zeros((features_test.shape[0],32,32,2))\n",
    "\n",
    "for k in range(features_test.shape[0]):\n",
    "    count = 0\n",
    "    for i in range(32):\n",
    "        for j in range(i,32):\n",
    "            X_test[k,i,j,0] = real_test[k,count]\n",
    "            X_test[k,i,j,1] = imag_test[k,count]\n",
    "            \n",
    "            if i!=j:\n",
    "                X_test[k,j,i,0] = X_test[k,i,j,0]\n",
    "                X_test[k,j,i,1] = -X_test[k,i,j,1]\n",
    "                \n",
    "            count = count + 1\n",
    "    \n",
    "    X_test[k,:,:,0] = X_test[k,:,:,0]/np.amax(np.abs(X_test[k,:,:,0]))\n",
    "    X_test[k,:,:,1] = X_test[k,:,:,1]/np.amax(np.abs(X_test[k,:,:,1]))                 \n",
    "\n",
    "\n",
    "for l in range(len(temp_ytest)):\n",
    "    y_test.append(str(temp_ytest[l]))\n",
    "    \n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.ravel()\n",
    "label_test = y_test.ravel()\n",
    "\n",
    "y_test = encode(y_test)\n",
    "y_test = pd.DataFrame.as_matrix(y_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "\n",
    "drate = 0.5\n",
    "n_node = 256\n",
    "batch_size = 128\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(input_shape=(32,32,2), filters=32, kernel_size=5, padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2), padding='same'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=5, activation='relu'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    #keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
    "    #keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    #keras.layers.BatchNormalization(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=n_node, activation='sigmoid'),\n",
    "    keras.layers.Dropout(drate),\n",
    "    keras.layers.Dense(units=95, activation='softmax')])\n",
    "\n",
    "lr = 0.001\n",
    "optimizer = keras.optimizers.Adam(lr)\n",
    "\n",
    "model.compile(optimizer,loss, metrics=['accuracy'])\n",
    "filepath = 'cnnc_model.hdf5'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, mode='auto',period=1)\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='auto')\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, mode='auto',restore_best_weights=True)\n",
    "callbacks_list = [checkpoint,reduce,early]\n",
    "\n",
    "t = time.time()\n",
    "infdb = model.fit(X_train, y_train, batch_size,verbose = True, epochs=1, validation_split=0.0, validation_data=(X_test,y_test), shuffle=True, callbacks=callbacks_list)\n",
    "elapsed = time.time() - t\n",
    "print(elapsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "train_loss, train_acc = model.evaluate(X_train,y_train)\n",
    "\n",
    "print('Test accuracy:', test_acc)\n",
    "print('Training accuracy:', train_acc)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "pred_labels = []\n",
    "for i in np.argmax(predictions, axis=1):\n",
    "    pred_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(1,67)\n",
    "fig1 = plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.suptitle('SNR = INF')\n",
    "plt.plot(epoch, infdb.history['loss'], label = 'Training')\n",
    "plt.plot(epoch, infdb.history['val_loss'],label = 'Testing')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.axis([1, 100, 0, 5])\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch, 100.*np.array(infdb.history['acc']),label = 'Training')\n",
    "plt.plot(epoch, 100.*np.array(infdb.history['val_acc']),label = 'Testing')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Percent Accuracy')\n",
    "plt.grid()\n",
    "plt.axis([1, 100, 0, 100])\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort output for plotting\n",
    "\n",
    "floaty_test = label_test.astype(np.float)\n",
    "inds = floaty_test.argsort()\n",
    "sorted_y_test = label_test[inds]\n",
    "\n",
    "sorted_y_pred = np.array(pred_labels)[inds]\n",
    "\n",
    "floatlabels = np.array(labels).astype(np.float)\n",
    "sortedlabels = np.sort(floatlabels)\n",
    "sortedclassnames = sortedlabels.astype(np.str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix to see where prediction errors occur\n",
    "\n",
    "cm = confusion_matrix(sorted_y_test, sorted_y_pred, sortedclassnames)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#np.fill_diagonal(cm, 0)\n",
    "\n",
    "index = np.linspace(3,50,95)\n",
    "index_str = []\n",
    "for i in range(len(index)):\n",
    "    index_str.append(str(index[i]))\n",
    "\n",
    "df_cm = pd.DataFrame(cm)\n",
    "fig1 = plt.figure(figsize = (20,14))\n",
    "sn.heatmap(df_cm,cmap=\"YlGnBu\",xticklabels=sortedclassnames,yticklabels=sortedclassnames,linewidths=.3)\n",
    "plt.xlabel=('True label'),\n",
    "plt.ylabel=('Predicted label')\n",
    "plt.title('SNR = INF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model('cnnc_1_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
