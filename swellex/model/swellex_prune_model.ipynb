{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def encode(series):\n",
    "    return pd.get_dummies(series.astype(str))\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(np.squeeze(y_true)), np.array(np.squeeze(y_pred))\n",
    "    return np.mean(np.abs((y_true - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('cnnr_109hz2_small1.h5')\n",
    "model.summary()\n",
    "weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model.layers[1].get_weights()[0]\n",
    "w2 = model.layers[4].get_weights()[0]\n",
    "w3 = model.layers[7].get_weights()[0]\n",
    "#w4 = model.layers[6].get_weights()[0]\n",
    "#w5 = model.layers[9].get_weights()[0]\n",
    "#w6 = model.layers[10].get_weights()[0]\n",
    "#w4 = model.layers[12].get_weights()[0]\n",
    "\n",
    "conv_layer_weights = [w1,w2,w3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filt_vec = np.zeros((3,256))\n",
    "\n",
    "for i in range(len(conv_layer_weights)):\n",
    "    weight=conv_layer_weights[i]\n",
    "    weights_dict = {}\n",
    "    num_filters = len(weight[0,0,0,:])\n",
    "    \n",
    "    for j in range(num_filters):\n",
    "        ws = np.sum(abs(weight[:,:,:,j]))\n",
    "        filt = '{}'.format(j)\n",
    "        weights_dict[filt]=ws\n",
    "    \n",
    "    weights_dict_sort = sorted(weights_dict.items(),key=lambda kv:kv[1])\n",
    "    print('L1 norm conv layer{}/n'.format(i+1),weights_dict_sort)\n",
    "    \n",
    "    weights_value = []\n",
    "    n = 0\n",
    "    for elem in weights_dict_sort:\n",
    "        weights_value.append(elem[1])\n",
    "        filt_vec[i,n] = elem[0]\n",
    "        n = n+1\n",
    "        \n",
    "    xc = range(num_filters)\n",
    "    \n",
    "    plt.figure(i+1,figsize=(7,5))\n",
    "    plt.plot(xc, weights_value)\n",
    "    plt.xlabel('filter_num')\n",
    "    plt.ylabel('L1 norm')\n",
    "    plt.title('conv layer{}'.format(i+1))\n",
    "    plt.grid(True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerassurgeon import identify, Surgeon\n",
    "from kerassurgeon.operations import delete_channels, delete_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = []\n",
    "for i in range(len(filt_vec[0,0:4])):\n",
    "    li1.append(int(filt_vec[0,i]))\n",
    "\n",
    "li2 = []\n",
    "for i in range(len(filt_vec[1,0:20])):\n",
    "    li2.append(int(filt_vec[1,i]))\n",
    "\n",
    "li3 = []\n",
    "for i in range(len(filt_vec[2,0:50])):\n",
    "    li3.append(int(filt_vec[2,i]))\n",
    "#li4 = []\n",
    "#for i in range(len(filt_vec[3,0:4])):\n",
    "#    li4.append(int(filt_vec[3,i]))\n",
    "\n",
    "#li5 = []\n",
    "#for i in range(len(filt_vec[4,0:6])):\n",
    "#    li5.append(int(filt_vec[4,i]))\n",
    "#li6 = []\n",
    "#for i in range(len(filt_vec[5,0:4])):\n",
    "#    li6.append(int(filt_vec[5,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model.layers[1]\n",
    "layer2 = model.layers[4]\n",
    "layer3 = model.layers[7]\n",
    "#layer4 = model.layers[6]\n",
    "#layer5 = model.layers[9]\n",
    "#ayer6 = model.layers[10]\n",
    "\n",
    "surgeon = Surgeon(model)\n",
    "#surgeon.add_job('delete_channels',layer1,channels=li1)\n",
    "#surgeon.add_job('delete_channels',layer2,channels=li2)\n",
    "#surgeon.add_job('delete_channels',layer3,channels=li3)\n",
    "#surgeon.add_job('delete_channels',layer4,channels=li4)\n",
    "#surgeon.add_job('delete_channels',layer5,channels=li5)\n",
    "#surgeon.add_job('delete_channels',layer6,channels=li6)\n",
    "\n",
    "model_new = surgeon.operate()\n",
    "lr = 0.00001\n",
    "loss='categorical_crossentropy'\n",
    "optimizer = keras.optimizers.Adam(lr)\n",
    "model_new.compile(optimizer,loss, metrics=['mean_absolute_error'])\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification\n",
    "\n",
    "#Import label data\n",
    "import pandas as pd\n",
    "temp_labels = np.loadtxt('vec_mat_clabels_swellex_0.01trainbb.csv',delimiter=\",\")\n",
    "labels_t = []\n",
    "\n",
    "for l in range(len(temp_labels)):\n",
    "    labels_t.append(str(temp_labels[l]))\n",
    "\n",
    "labels_t = np.array(labels_t)\n",
    "labels_t = labels_t.ravel()\n",
    "\n",
    "def encode(series):\n",
    "    return pd.get_dummies(series.astype(str))\n",
    "\n",
    "y_train = encode(labels_t)\n",
    "labels = list(y_train.columns.values)\n",
    "\n",
    "y_train = pd.DataFrame.as_matrix(y_train)\n",
    "\n",
    "#Import test data\n",
    "\n",
    "features_test1 = np.loadtxt('vec_mat_features_swellex_109hz2_test1.csv',delimiter=\",\")\n",
    "#features_test2 = np.loadtxt('vec_mat_features_swellex_test2_163hz.csv',delimiter=\",\")\n",
    "#features_test3 = np.loadtxt('vec_mat_features_swellex_test2_232hz.csv',delimiter=\",\")\n",
    "#features_test4 = np.loadtxt('vec_mat_features_swellex_test2_385hz.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_swellex_109hz2_test.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "featurestest_mat = np.array([[features_test1]])#,[features_test2],[features_test3],[features_test4]])\n",
    "X_test = np.zeros((features_test1.shape[0],21,21,2))\n",
    "\n",
    "for f in range(featurestest_mat.shape[0]):\n",
    "    print(f)\n",
    "    features_test = featurestest_mat[f,0,:,:]\n",
    "    real_test = features_test[:,0::2]\n",
    "    imag_test = features_test[:,1::2]\n",
    "\n",
    "    for k in range(features_test.shape[0]):\n",
    "        count = 0\n",
    "        for i in range(21):\n",
    "            for j in range(i,21):\n",
    "                X_test[k,i,j,2*f] = real_test[k,count]\n",
    "                X_test[k,i,j,2*f+1] = imag_test[k,count]\n",
    "            \n",
    "                if i!=j:\n",
    "                    X_test[k,j,i,2*f] = X_test[k,i,j,2*f]\n",
    "                    X_test[k,j,i,2*f+1] = -X_test[k,i,j,2*f+1]\n",
    "            \n",
    "                count = count + 1\n",
    "\n",
    "#X_test[k,:,:,0] = X_test[k,:,:,0]/np.amax(np.abs(X_test[k,:,:,0]))\n",
    "#X_test[k,:,:,1] = X_test[k,:,:,1]/np.amax(np.abs(X_test[k,:,:,1]))\n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "#y_test = (temp_ytest - mu)/sigma_labels\n",
    "y_test = temp_ytest\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "#Classification acc\n",
    "pred_labels = []\n",
    "for i in np.argmax(predictions, axis=1):\n",
    "    pred_labels.append(float(labels[i]))\n",
    "    \n",
    "#print(np.shape(pred_labels))\n",
    "\n",
    "\n",
    "mae = mean_absolute_error(y_test, pred_labels)\n",
    "diff = abs((np.transpose(pred_labels))-(y_test))\n",
    "error = diff[diff>1]\n",
    "percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "\n",
    "print('Test mae:', mae)\n",
    "print('Testing percent within 1km:', percent_correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new.save('cnnc_109hz2_small5.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "class traininghist(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        self.trainingloss = []\n",
    "        self.trainingmape = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, mape = self.model.evaluate(x, y, verbose=0)\n",
    "        self.trainingloss.append(loss)\n",
    "        self.trainingmape.append(mape)\n",
    "        print('Training loss: {}, mape: {}\\n'.format(loss, mape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training data\n",
    "\n",
    "features1 = np.loadtxt('vec_mat_features_swellex_0.01trainbb_109hz.csv',delimiter=\",\")\n",
    "features2 = np.loadtxt('vec_mat_features_swellex_0.01trainbb_163hz.csv',delimiter=\",\")\n",
    "features3 = np.loadtxt('vec_mat_features_swellex_0.01trainbb_232hz.csv',delimiter=\",\")\n",
    "features4 = np.loadtxt('vec_mat_features_swellex_0.01trainbb_385hz.csv',delimiter=\",\")\n",
    "labels_unstand = np.loadtxt('vec_mat_rlabels_swellex_0.01trainbb.csv',delimiter=\",\")\n",
    "labels_t = []\n",
    "\n",
    "features_mat = np.array([[features1],[features2],[features3],[features4]])\n",
    "X_train = np.zeros((features1.shape[0],21,21,8))\n",
    "\n",
    "for f in range(features_mat.shape[0]):\n",
    "    print(f)\n",
    "    features = features_mat[f,0,:,:]\n",
    "    real = features[:,0::2]\n",
    "    imag = features[:,1::2]\n",
    "    \n",
    "    for k in range(features.shape[0]):\n",
    "        count = 0\n",
    "        for i in range(21):\n",
    "            for j in range(i,21):\n",
    "                X_train[k,i,j,2*f] = real[k,count]\n",
    "                X_train[k,i,j,2*f+1] = imag[k,count]\n",
    "                \n",
    "                if i!=j:\n",
    "                    X_train[k,j,i,2*f] = X_train[k,i,j,2*f]\n",
    "                    X_train[k,j,i,2*f+1] = -X_train[k,i,j,2*f+1]\n",
    "                \n",
    "                count = count + 1\n",
    "\n",
    "#X_train[k,:,:,0] = X_train[k,:,:,0]/np.amax(np.abs(X_train[k,:,:,0]))\n",
    "#X_train[k,:,:,1] = X_train[k,:,:,1]/np.amax(np.abs(X_train[k,:,:,1]))\n",
    "\n",
    "labels_unstand = labels_unstand.ravel()\n",
    "#y_train,mu,sigma_labels = std_y(labels_unstand)\n",
    "\n",
    "y_train = labels_unstand\n",
    "y_train = y_train+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import test data\n",
    "\n",
    "features_test1 = np.loadtxt('vec_mat_features_swellex_test2_109hz.csv',delimiter=\",\")\n",
    "features_test2 = np.loadtxt('vec_mat_features_swellex_test2_163hz.csv',delimiter=\",\")\n",
    "features_test3 = np.loadtxt('vec_mat_features_swellex_test2_232hz.csv',delimiter=\",\")\n",
    "features_test4 = np.loadtxt('vec_mat_features_swellex_test2_385hz.csv',delimiter=\",\")\n",
    "temp_ytest = np.loadtxt('vec_mat_rlabels_swellex_test2.csv',delimiter=\",\")\n",
    "y_test= []\n",
    "\n",
    "featurestest_mat = np.array([[features_test1],[features_test2],[features_test3],[features_test4]])\n",
    "X_test = np.zeros((features_test1.shape[0],21,21,8))\n",
    "\n",
    "for f in range(featurestest_mat.shape[0]):\n",
    "    print(f)\n",
    "    features_test = featurestest_mat[f,0,:,:]\n",
    "    real_test = features_test[:,0::2]\n",
    "    imag_test = features_test[:,1::2]\n",
    "\n",
    "    for k in range(features_test.shape[0]):\n",
    "        count = 0\n",
    "        for i in range(21):\n",
    "            for j in range(i,21):\n",
    "                X_test[k,i,j,2*f] = real_test[k,count]\n",
    "                X_test[k,i,j,2*f+1] = imag_test[k,count]\n",
    "            \n",
    "                if i!=j:\n",
    "                    X_test[k,j,i,2*f] = X_test[k,i,j,2*f]\n",
    "                    X_test[k,j,i,2*f+1] = -X_test[k,i,j,2*f+1]\n",
    "            \n",
    "                count = count + 1\n",
    "\n",
    "#X_test[k,:,:,0] = X_test[k,:,:,0]/np.amax(np.abs(X_test[k,:,:,0]))\n",
    "#X_test[k,:,:,1] = X_test[k,:,:,1]/np.amax(np.abs(X_test[k,:,:,1]))\n",
    "\n",
    "temp_ytest = temp_ytest.ravel()\n",
    "#y_test = (temp_ytest - mu)/sigma_labels\n",
    "y_test = temp_ytest\n",
    "y_test = y_test+5\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.00001\n",
    "loss='mean_squared_error'\n",
    "optimizer = keras.optimizers.Adam(lr)\n",
    "batch_size = 128\n",
    "\n",
    "model.compile(optimizer,loss, metrics=['mape'])\n",
    "\n",
    "filepath = \"temp.h5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, mode='auto',period=1)\n",
    "reduce = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=75, mode='auto')\n",
    "early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=125, mode='auto',restore_best_weights=True)\n",
    "traininghistory = traininghist((X_train,y_train))\n",
    "callbacks_list = [checkpoint,reduce,traininghistory,early]\n",
    "\n",
    "infdb = model_new.fit(X_train, y_train, batch_size,verbose = True, epochs=5000, validation_split=0.2, shuffle=True,callbacks=callbacks_list)\n",
    "\n",
    "#Testing\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "train_loss, train_acc = model.evaluate(X_train,y_train)\n",
    "\n",
    "y_train = y_train-5\n",
    "y_test = y_test-5\n",
    "\n",
    "predictions = model_new.predict(X_test)\n",
    "predictions = predictions-5\n",
    "diff = abs((np.transpose(predictions))-(y_test))\n",
    "error = diff[diff>1]\n",
    "percent_correct = (len(y_test)-len(error))/len(y_test)\n",
    "\n",
    "predictions_train = model_new.predict(X_train)\n",
    "predictions_train = predictions_train-5\n",
    "diff = abs((np.transpose(predictions_train))-(y_train))\n",
    "error = diff[diff>1]\n",
    "percent_correct_train = (len(y_train)-len(error))/len(y_train)\n",
    "\n",
    "print('Training mape:', train_acc)\n",
    "print('Test mape:', test_acc)\n",
    "print('Training percent within 1km:',percent_correct_train*100)\n",
    "print('Testing percent within 1km:', percent_correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
